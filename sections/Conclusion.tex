\chapter{Conclusion}

\section{Summary of achievement}

The aim of this project was to provide a distilled and coherent representation of the news in a particular industry, to provide valuable insight into the pertinent themes, events and entities in a particular category of news for a set period of time. Our work has succeeded in doing so by developing a tool that provides cohesive semantic analysis of news by grouping articles in a specific category (such as Travel, Business, Markets, etc.) into semantic clusters, on which latent topics are modelled, using LDA, to summarise the key themes in these semantic clusters (See~\Cref{ch:4:topic}). 
For topic extraction, usually one of either cluster analysis or topic modelling is performed on the input data (news articles), however, by combining these methods we are able to provide a high-level semantic grouping through clustering as well as a finer-grained grouping through topic models, improving the coherence of these the extracted latent topics.
 Using POS filtering, stopword removal, lemmatisation and TF-IDF to pre-process the LDA corpus allows for distinct topics with minimal overlap in the semantic clusters. Additionally, our tool performs semantic triple extraction for each topic in a semantic cluster to obtain a minimal representation of the information provided by articles (associated with the topic) such as the `key entities', their relationships to events and other entities as well as the sentiment of the news surrounding them (\Cref{ch:5:triple}).  

Based on both quantitative empirical results (such as silhouette and coherence scores) as well as qualitative user feedback, the final product gives confidence in the extraction relevant information though topics and triples and displays these results through graphs generated by visualisation tool in the end-to-end semantic analysis engine (See \Cref{fig:sys_arch}). By carrying out extensive research into related works for similar problems, as well as a thorough investigation and evaluative to optimise the different components of the semantic analysis engine in terms of quality of results and time efficiency, we were able to design a novel system which combines several semantic analysis techniques in order to develop a user-friendly, end-to-end pipeline for visualising the news automatically.


% The product successfully met the initial four goals to

% textual entailment to measure
% agreement between articles from different sources.

% Key learnings were very broad, ranging from building scrapers to concurrently collect data from different news
% sources, to conducting complex NER and SRL analysis, to developing user functionality on top of a large network of
% nodes in a force-directed graph.
\section{Wider applications}
The current solution for semantic analysis of news proposed in this project focuses on a single industry, i.e., airline. A wider application for this project could involve using the proposed tool across different industries combining to see how the news across industries affect each other and what common themes or topics, if any, can be extrapolated from the semantic analysis. This could be useful not only for the average consumer but also relevant from a business intelligence standpoint where companies can see, for example, market trends across industries such as airline, energy etc. or travel trends in hotel management and airline industry, getting an insight into how they are affected by each other.

Additionally, collecting more data across different years would be useful as it would allow more substantial temporal semantic analysis of the news, where the user can infer how the topics and content of the news (through triples) changes through different periods of time for new articles associated with a particular industry. 



\section{Future Work}

\begin{enumerate}
    % \item Topic Bert for modelling topics:
    \item \textbf{Sentence relevance for article summarisation:} The current approach for generating the semantic clusters and topics uses the `article intros' rather than the entire article (as they introduce a lot of redundancy). The `intros' consist of the first 8 sentences of the article as the introductions of the articles generally provide a good summary whilst retaining sufficient context. An alternative approach to the `article intros' involves obtaining the extractive text summary of the articles by selecting the most relevant sentences in the corpus through sentence ranking algorithms~\cite{jevzek2008automatic}~\cite{madhuri2019extractive}. 
    
    \item \textbf{Linking to existing Knowledge Base for Disambiguation and Data Augmentation:} The current approach simply relies on the entities extracted from processing raw text from the articles to generate the knowledge graph (of semantic triples). This has the potential of introducing redundancy as the aliases of named entities (e.g. BA and British Airways) do not get resolved to the same entity. A beneficial improvement for the future would be to use named entity linking (NEL)\cite{retrospective_kg} and store the semantic triples in an RDF triplestore where the `subject entity' is represented with a unique International Resource Identifier (IRI) \cite{internationalized}. This allows for Named Entity Disambiguation (See~\Cref{ned}) by removing the dependency of subject nodes on the aliases of named entities. 
    
    \item \textbf{Experiment with ELMo embeddings for document vectorisation.}  The challenges with approaches such as Word2Vec and GloVe word embeddings are that they provide a single context-independent representation for a word and struggle with ``out-of-vocabulary (OOV) words"~\cite{elmo_word_rep}, i.e., words that were never encountered prior will often be represented as a random vector which is not ideal. A potential improvemnt over this would be to use ELMo to allow context-dependant word embeddings (See \Cref{elmo}).
    
\end{enumerate}
