\chapter{Conclusion}
\vspace*{-1em}
\section{Summary of achievement}

The aim of this paper was to propose a solution that provides a distilled and coherent representation of the news for a particular industry, offering valuable insight into the pertinent themes, events, and entities within a specific genre of news for a set period of time. Our work has succeeded in this through our Semantic Analysis Engine, which focuses on two main aspects: topic extraction and information retrieval through semantic triple extraction. 

Our Topic Extraction engine performs cohesive semantic analysis of news for the airline industry by grouping the articles in a specific category (such as Travel, Business, etc.) into semantic clusters that are used to model topics (\Cref{ch:4:topic}). This engine uses a combination of popular methods, such as KMeans cluster analysis and latent topic modelling using LDA, to provide a high-level semantic grouping through clustering as well as a finer-grained grouping through topic models. We incorporate a range of different natural language processing techniques such as coreference resolution, POS tag filtering, stopword removal, lemmatisation, etc., and explore different article embedding approaches to improve the `quality' of semantic clustering by optimising for the silhouette score. Similarly, to model meaningful distinct topics, we optimise for the coherence scores.

Our Semantic Triple Extraction Engine exploits the rich morpho-syntactic structure of the English language and adopts a lexico-semantic approach using dependency parsing and POS tagging to provide a domain-independent solution (without using ontologies and an existing knowledge base) for triple extraction. The semantic triples extracted for each topic in a semantic cluster provide a minimal knowledge representation of the underlying information such as the `key entities', their relationships to events and other entities, as well as the sentiment of the news surrounding them (\Cref{ch:5:triple}).

Based on both quantitative results (such as silhouette and coherence scores) as well as qualitative user feedback (\Cref{ch:6:eval}), the final product gives confidence in the extraction of relevant information through topics and semantic triples and displays these results through graphs generated by the visualisation tool in our end-to-end semantic analysis engine (see \Cref{fig:sys_arch}). By carrying out extensive research into related works, as well as thorough investigation and evaluation to optimise the different components of our semantic analysis engine, we present a novel system that combines several NLP techniques to develop a user-friendly, end-to-end pipeline for visualising semantic analysis of news automatically.


% The product successfully met the initial four goals to

% textual entailment to measure
% agreement between articles from different sources.

% Key learnings were very broad, ranging from building scrapers to concurrently collect data from different news
% sources, to conducting complex NER and SRL analysis, to developing user functionality on top of a large network of
% nodes in a force-directed graph.
\section{Wider applications}
The current solution for semantic analysis of news proposed in this paper focuses on a single industry, i.e., the airline industry. A wider application for this project could involve using the proposed tool across different industries combining to see how the news across industries affects each other and what common themes or topics, if any, can be extrapolated from the semantic analysis. This could be useful not only for the average consumer but also relevant from a business intelligence standpoint where companies can gain insight into, for instance, the market trends across industries such as airline, energy etc., or travel trends in hotel management and airline industries.

Additionally, collecting more data across different years would be useful as it would allow for a more substantial temporal semantic analysis of the news associated with an industry, where the user can infer how the topics and content of the news (through triples) change through different periods of time.

\section{Future Work}

\begin{enumerate}
    % \item Topic Bert for modelling topics:
    \item \textbf{Sentence relevance for article summarisation:} The current approach for generating the semantic clusters and topics uses the `article intros' rather than the entire article (to avoid redundancy). The `intros' consist of the first 8 sentences of the article as the introductions of news articles generally provide a good summary whilst retaining sufficient context. An alternative approach to the `article intros' involves obtaining the extractive text summary of the articles by selecting the most relevant sentences in the corpus through sentence ranking algorithms~\cite{jevzek2008automatic}~\cite{madhuri2019extractive}. 
    
    \item \textbf{Linking to existing Knowledge Base for Disambiguation and Data Augmentation:} The current approach simply relies on the entities extracted from processing raw text from the articles to generate the knowledge graph (of semantic triples). This has the potential of introducing redundancy as the aliases of named entities (e.g., BA and British Airways) do not get resolved to the same entity. A beneficial improvement for the future would be to use named entity linking (NEL)~\cite{retrospective_kg} and store the semantic triples in an RDF triplestore where the `subject entity' is represented with a unique International Resource Identifier (IRI)~\cite{internationalized}. This allows for Named Entity Disambiguation (see~\Cref{ned}) by removing the dependency of subject nodes on the aliases of named entities. 
    
    \item \textbf{Experiment with ELMo embeddings for document vectorisation.}  The challenges with approaches such as Word2Vec and GloVe word embeddings are that they provide a single context-independent representation for a word and struggle with ``out-of-vocabulary (OOV) words"~\cite{elmo_word_rep}, i.e., words that were never encountered prior will often be represented as a random vector which is not ideal. A potential improvement over this would be to use ELMo to allow context-dependent word embeddings (see~\Cref{elmo}).
    
\end{enumerate}