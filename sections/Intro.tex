\chapter{Introduction}

In today's fast-growing economy, there is an incomprehensible amount of information from a host of different sources readily available at our disposal. This can be extremely overwhelming and difficult to navigate. There is a need for a concise and digestible medium of information that allows the consumer to gain insight into the major themes and events of the moment, how they relate to one another, and how they are likely to affect key industry players by means of data visualisation. Data visualisation can help make the data more accessible and available. \\

This project tries to solve this problem of information overload in the business and industry sectors (such as airline, energy and pharmaceutical industry) by providing a visualisation tool that dynamically depicts how the main contributors (companies, people, events) within an industry are affected by the news and what the general sentiment surrounding these contributors are. \\

Such a product has great value and interest for not only the average civilian who does not want to sift through reams of news to gain a core insight into the developments within an industry but also for the businesses that want to obtain an intelligent overview of workings within specific industries. \\


% The general idea in modelling this 'visualiser' entails a multitude of tasks. First of which, is identifying the key entities in relation to an industry by making use of entity extraction and building entity-relationship ontologies. These can be derived from large open-source knowledge bases (or knowledge graphs) such as Wikidata by leveraging the knowledge graph embeddings such as semantic triples. \hyperlink{8}{[8]}\hyperlink{9}{[9]} This can then, in turn, be visualised as a more succinct and relevant knowledge graph, now only showing selective information applicable to an industry. \\

% Next step is to apply Natural Language Processes (NLP) processes such as Named Entity Extraction (NER) for mining news articles to, again, identify the key entities in the industry, ma,king sure to avoid duplication and redundancy in these entities by using Named Entity Disambiguation (NED) and finally linking these entities to the knowledge graph using Named Entity Linking (NEL). The information from processing these news articles will augment and alter our knowledge graph based on the information obtained from these articles. There is an immense amount of research done on natural language processing using machine learning for mining data for entity extraction from articles \hyperlink{11}{[11]} and a range of existing state-of-the-art machine learning models such as ELMo-LSTM \hyperlink{31}{[31]} and RoBERTa developed by AllenNLP which are open-source. I will be using these models, evaluating their comparative performance and potentially using a hybrid of such models to best suit the needs of the project. \\

% Extracting the key entities and relationships from the news is useful but doesn't provide an overview of themes and events that dominate an industry during a period of time. This can be achieved by obtaining the sentiment and key themes seen in the news of a particular industry by using NLP processes like Sentiment Analysis \hyperlink{18}{[18]}\hyperlink{20}{[20]}\hyperlink{22}{[22]}\hyperlink{24}{[24]} and Topic Modelling \hyperlink{23}{[23]} respectively. Again, there is a range of research and pre-existing state-of-the-art sentiment analyser models, AllenNLP models can again be leveraged for this and evaluated in a similar fashion as those for entity extraction. \\

% The last milestone is to present all this information, the knowledge graph of entities, major themes/ topics and their associated sentiment, in an intelligent, interpretable and informative manner. This can be a particularly challenging task as it requires a lot of different considerations to be taken into account such as filtering what needs to be displayed, how it should be displayed and how to dynamically change the display based on new relevant information. Additionally, it can be challenging to identify what constitutes as a 'effective' visualisation as that can be fairly subjective. There is a range of different techniques and libraries that aid to data visualisation including libraries such as D3.js, Dash to build various types for graph visualisations as well as visual analyser clustering machine learning models such as Self-Organising Maps (Kohonen Maps). These different approaches allow for different capabilities and in turn, different ways of visualising the data. \\

% While there are numerous studies and research on these different aspects of the project, they usually approach a standalone use case e.g., getting sentiment from data. There is a need for integrating the different aspects of information mining and delivering cohesive visual platform for this information, allowing the user to see the changes in these industries over time. 

\section{Motivation}

\section{Objectives} \label{objectives}


\section{Contributions}



\todonum[inline, color=lightgray, textcolor=black]{ Background: LDA}
\todonum[inline, color=lightgray, textcolor=black]{https://link.springer.com/content/pdf/10.1186/s13673-019-0192-7.pdf : FOR TF-IDF and BoW}

\todonum[inline, color=lightgray, textcolor=black]{ Background: Remove visualisation}