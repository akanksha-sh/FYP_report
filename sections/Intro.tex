\chapter{Introduction}
\vspace{-2ex}

\chapquote{``We are assaulted with more information than any one of us can handle".}{Daniel Levitin}{cognitive psychologist and neuroscientist}

In today's fast-growing, data-driven economy, there is an incomprehensible amount of information available at our disposal. Thousands of articles are written every day and made available through a variety of internet outlets. For an average consumer, this can be extremely overwhelming and difficult to navigate. This poses a need for a concise and digestible medium of information that allows consumers to acquire insight into significant themes, events, and entities around their areas of interest, perhaps throughout a specific time period, without having to wade through reams of superfluous articles. This prompts performing semantic analysis on the news articles, in order to extract relevant and distilled information from the news corpora. 

\section{Motivation}

Information extraction or retrieval from online news corpora is huge area of research in the natural language processing (NLP) community. With an overwhelming amount of information available, the main task in information retrieval entails automatic extraction of structured information such as entities, relationships between entities, etc., from unstructured sources. This is achieved through syntactic parsing, which exploits the structure of the natural language model focusing on lexical meaning and part-of-speech, and semantic analysis, which features the meaning of text in the corpus~\cite{global_ents_intro}~\cite{sarawagi_info}. Early works in this area involved performing identification and temporal analysis of named entities, such as persons, organisations etc., their co-occurrence in news articles~\cite{sarawagi_info}~\cite{intro_semntic_analysis_news}, event detection~\cite{finance}~\cite{finance_events} and information distillation by news summarisation, sentence relevance identification~\cite{news_info_extract}~\cite{summary_generation_intro}. A more interesting way of extracting information involves generating semantic triples of type (subject, predicate, object) from news articles to generate a knowledge graph, which serves as a minimal representation of the information presented in news articles, whilst still retaining sufficient context. Previous works on this approach, however, rely on using ontologies and augmenting existing knowledge bases for triple extraction, making them domain-dependant~\cite{wu2020knowledge}. Our work explores the domain-independent approach which uses the rich morpho-syntactic marking system of English (verb inflections, clausal markers, nominal case)~\cite{tseng2014chinese} to identity the structure of triples. 

Another prevalent area of research in semantic analysis of news articles follows topic modelling, which involves extracting latent topics from the news article corpora using multinomial distributions over a fixed vocabulary~\cite{nouns_only_lda}. The motivation for this stems from extracting common themes in large corpus of articles and is often combined with the another NLP approach: sentiment analysis, to provide an insight into the sentiment surrounding the latent topics. Common approaches for this include using Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) models. Alternatively, clustering methods are also quite common to group articles into topics in an attempt to achieve a high-level similarity clusters of articles~\cite{clustering_intro} that rely more heavily on the semantic meaning of the words rather than probabilistic distributions of keywords (as seen in LDA). 

% knowledge distallition 
% lexico-semantic 
\section{Objectives} \label{objectives}

Our work draws on the techniques discussed above to provide a semantic analysis engine, that aims to help alleviate information overload by distilling the knowledge extracted from the news corpora (into topics and corresponding semantic triples). For the scope of this project, the news is limited to the airline industry. The objectives for constructing the semantic analysis engine are as follows: 

% that groups semantically similar articles and extracts latent topics (and associated sentiment) to indicate key themes in the clustered article corpus. Additionally, it extracts semantic triples from each of these latent topics in the semantic clusters to provide an overview of the entities, their relationships, the general sentiment around events and news surrounding them.


\begin{enumerate}
    \item \textbf{Cluster articles based on the semantic similarity of their text.} Vectorise articles experimenting with different embeddings and vectorisation techniques in order to generate semantic document vectors for clustering analysis. 

    \item \textbf{Topic Modelling on semantic clusters to obtain information on latent topics.} Generate latent topic models to indicate key themes in the clustered article corpus, experimenting with different processing techniques on the input corpus to improve coherence of the latent topic models.

    \item \textbf{Extraction of semantic triples for each topic in cluster.} Extract semantic triples to provide an overview of the entities, their relationships, and the general sentiment around events and news surrounding them.

    \item \textbf{Visualisation of results from information extraction} Display the results of the information extracted from the semantic analysis tool, i.e., semantic clusters, latent topics and triples, in a cohesive visualisation.  
    
\end{enumerate}

\section{Contributions} \label{contributions}

This project focuses on building a cohesive semantic analysis tool, that incorporates the different semantic analysis approaches and has the following contributions: 
 
\begin{enumerate}
    \item \textbf{Topic Extraction Engine:} A novel engine that performs semantic clustering of news articles experimenting with different pre-processing techniques and document vector generation approaches the influence corpus decisions and uses KMeans++ as the clustering technique. The resulting semantic clusters then undergo topic modelling (using LDA) to extract latent topics (optimised for coherence) as well as corresponding articles and sentiment for each topic (See~\Cref{ch:4:topic}).

    \item \textbf{Semantic Triple Extraction Engine:} This engine focuses on extracting semantic triples of type (subject $\rightarrow$ predicate $\rightarrow$ object) focusing on the named entities and information surrounding them, inferring the average sentiment of the entity through triple sentiment (See~\Cref{ch:5:triple}).
  
    \item \textbf{Visualisation Tool:} A web-based application responsible for illustrating the results from the Topic Extraction Engine and Semantic Triple Extraction Engine in a coherent and cohesive visualisation by making use of circle-packing topic cluster diagrams and a force-directed knowledge graphs respectively. 


\end{enumerate}


Such a product has great value and interest for not only the average consumer who does not want to sift through reams of news to gain a core insight into the developments within an industry but also for businesses that want to obtain an intelligent overview of key information within specific industries.

% way of evaluation 
% ----------------------------------------

% The general idea in modelling this 'visualiser' entails a multitude of tasks. First of which, is identifying the key entities in relation to an industry by making use of entity extraction and building entity-relationship ontologies. These can be derived from large open-source knowledge bases (or knowledge graphs) such as Wikidata by leveraging the knowledge graph embeddings such as semantic triples. \hyperlink{8}{[8]}\hyperlink{9}{[9]} This can then, in turn, be visualised as a more succinct and relevant knowledge graph, now only showing selective information applicable to an industry. \\

% Next step is to apply Natural Language Processes (NLP) processes such as Named Entity Extraction (NER) for mining news articles to, again, identify the key entities in the industry, ma,king sure to avoid duplication and redundancy in these entities by using Named Entity Disambiguation (NED) and finally linking these entities to the knowledge graph using Named Entity Linking (NEL). The information from processing these news articles will augment and alter our knowledge graph based on the information obtained from these articles. There is an immense amount of research done on natural language processing using machine learning for mining data for entity extraction from articles \hyperlink{11}{[11]} and a range of existing state-of-the-art machine learning models such as ELMo-LSTM \hyperlink{31}{[31]} and RoBERTa developed by AllenNLP which are open-source. I will be using these models, evaluating their comparative performance and potentially using a hybrid of such models to best suit the needs of the project. \\

% Extracting the key entities and relationships from the news is useful but doesn't provide an overview of themes and events that dominate an industry during a period of time. This can be achieved by obtaining the sentiment and key themes seen in the news of a particular industry by using NLP processes like Sentiment Analysis \hyperlink{18}{[18]}\hyperlink{20}{[20]}\hyperlink{22}{[22]}\hyperlink{24}{[24]} and Topic Modelling \hyperlink{23}{[23]} respectively. Again, there is a range of research and pre-existing state-of-the-art sentiment analyser models, AllenNLP models can again be leveraged for this and evaluated in a similar fashion as those for entity extraction. \\

% The last milestone is to present all this information, the knowledge graph of entities, major themes/ topics and their associated sentiment, in an intelligent, interpretable and informative manner. This can be a particularly challenging task as it requires a lot of different considerations to be taken into account such as filtering what needs to be displayed, how it should be displayed and how to dynamically change the display based on new relevant information. Additionally, it can be challenging to identify what constitutes as a 'effective' visualisation as that can be fairly subjective. There is a range of different techniques and libraries that aid to data visualisation including libraries such as D3.js, Dash to build various types for graph visualisations as well as visual analyser clustering machine learning models such as Self-Organising Maps (Kohonen Maps). These different approaches allow for different capabilities and in turn, different ways of visualising the data. \\

% While there are numerous studies and research on these different aspects of the project, they usually approach a standalone use case e.g., getting sentiment from data. There is a need for integrating the different aspects of information mining and delivering cohesive visual platform for this information, allowing the user to see the changes in these industries over time. 

\section{Ethical Considerations} 
This project focuses on semantic analysis of news articles with aim of knowledge distillation which from an intelligence standpoint, is extremely useful as it condenses large amounts of news to give a visual overview of an industry in a specific category over a period of time. Though this project might highlight recent developments and trends in a sector, it is extremely unlikely to influence and propagate some radicalised opinion. Additionally, given that there is no direct human involvement in this project, there is no collection or direct use of personal data.

One important ethical consideration, however, is to think about any bias introduced in the semantic analysis engine due to the nature of the input data which inherently might contain some underlying bias. For example, using the engine with a different input dataset focusing on the energy industry, which consists of majority right-wing articles talking about ``climate change being a hoax' and refusing to support clean energy alternatives, then that will be reflected in the output results (topics, associated sentiments and semantic triples) via the visualisation tool, illustrating a negative sentiment associated with clean energy. This is the nature of news publication, which is more often than not biased and polarising. This makes it very difficult to gauge what quantifies as unbiased, in fact, performing any filtration of articles to ``alleviate bias" would inherently introduce bias.

Another key ethical consideration is that of using software and data that may have copyright licensing implications. A key aspect of this project is to use news articles for entity, sentiment and topic extraction. The news articles dataset is provided by Deep Search Labs and are scraped from online sources, in particular, Bloomberg News. Using the articles (which are an Intellectual Property (IP) of these companies) for ``non-commercial" data analysis is not a copyright infringement as the copyright law has been updated to provide an exception for ``Text and data mining technologies to help researchers process large amounts of data"~\cite{exceptions_to_copyright}. All other libraries and software packages used such as D3, spaCy, Gensim and AllenNLP (See~\Cref{libraries}) are open-source and free to use for the purposes of this project (i.e., for academic use).