\chapter{Introduction}
\vspace{-2ex}

\chapquote{``We are assaulted with more information than any one of us can handle".}{Daniel Levitin}{cognitive psychologist and neuroscientist}

In today's fast-growing, data-driven economy, there is an incomprehensible amount of information available at our disposal. Thousands of articles are written every day and made available through a variety of internet outlets. For the average consumer, this can be extremely overwhelming and difficult to navigate. This poses a need for a concise and digestible medium of information that allows consumers to acquire insight into significant themes, events, and entities around their areas of interest, perhaps for a specific time period, without having to wade through reams of superfluous articles. This prompts performing semantic analysis on the news articles to extract relevant and distilled information from the news corpora.

\section{Motivation}

Information extraction or retrieval from online news corpora is a huge area of research in the natural language processing (NLP) field. With an overwhelming amount of information available, the main task in information retrieval entails the automatic extraction of structured information such as entities, relationships between entities, etc., from unstructured sources. This is achieved through syntactic parsing, which exploits the structure of the natural language model, focusing on lexical meaning and part-of-speech, and semantic analysis, which features the meaning of a text in the corpus~\cite{global_ents_intro}~\cite{sarawagi_info}. Early research in this area focused on temporal analysis of co-occurrence of named entities (e.g., persons, organisations, etc.)~\cite{sarawagi_info}~\cite{intro_semntic_analysis_news}, event detection~\cite{finance}~\cite{finance_events}, and information distillation through news summarisation and sentence relevance identification~\cite{news_info_extract}~\cite{summary_generation_intro}. A more interesting way of extracting information involves generating semantic triples of type (subject, predicate, object) from news articles to generate a knowledge graph, which serves as a minimal representation of the information presented in news articles, whilst still retaining sufficient context. Previous works on this approach, however, rely on using ontologies and augmenting existing knowledge bases for triple extraction, making them domain-dependant~\cite{wu2020knowledge}. Our work explores the domain-independent approach which uses the rich morpho-syntactic marking system of English (verb inflexions, clausal markers, nominal case)~\cite{tseng2014chinese} to identify the structure of triples.

Another prevalent area of research in semantic analysis of news articles follows topic modelling, which involves extracting latent topics from the news article corpora using multinomial distributions over a fixed vocabulary~\cite{nouns_only_lda}. The motivation for this stems from extracting common themes in a large corpus of articles and is often combined with another NLP approach: sentiment analysis, to provide an insight into the sentiment surrounding the latent topics. Common approaches for this include using Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) models. Alternatively, clustering methods are also quite common to group articles into topics in an attempt to achieve high-level similarity clusters of articles~\cite{clustering_intro} that rely more heavily on the semantic meaning of the words rather than probabilistic distributions of keywords (as seen in LDA).

% knowledge distallition
% lexico-semantic
\section{Objectives} \label{objectives}

Our work draws on the techniques discussed above to provide a semantic analysis engine that aims to help alleviate information overload by distilling the knowledge extracted from the news corpora (into topics and corresponding semantic triples). For the scope of this project, the news is limited to the airline industry. The objectives for constructing the semantic analysis engine are as follows:

% that groups semantically similar articles and extracts latent topics (and associated sentiment) to indicate key themes in the clustered article corpus. Additionally, it extracts semantic triples from each of these latent topics in the semantic clusters to provide an overview of the entities, their relationships, the general sentiment around events and news surrounding them.


\begin{enumerate}
    \item \textbf{Cluster articles based on the semantic similarity of their text.} Vectorise articles experimenting with different embeddings and vectorisation techniques to generate semantic document vectors for clustering analysis.

    \item \textbf{Topic Modelling on semantic clusters to obtain information on latent topics.} Generate latent topic models to indicate key themes in the clustered article corpus, experimenting with different processing techniques on the input corpus to improve coherence of the latent topic models.

    \item \textbf{Extraction of semantic triples for each topic in cluster.} Extract semantic triples to provide an overview of the entities, their relationships, and the general sentiment around events and news surrounding them.

    \item \textbf{Visualisation of results from information extraction.} Display the results of the information extracted from the semantic analysis tool, i.e., semantic clusters, latent topics and triples, in a cohesive visualisation.

\end{enumerate}

\section{Contributions} \label{contributions}

This paper presents an end-to-end cohesive semantic analysis pipeline, that incorporates the different semantic analysis approaches and has the following key contributions:

\begin{enumerate}
    \item \textbf{A novel process for topic extraction in a news corpus}: We propose our Topic Extraction Engine, which generates clusters of news articles based on the semantic similarity of their text. The generated semantic clusters are then subjected to topic modelling (through LDA) to extract latent topic models, which are optimised for coherence. For each of these topics, the engine infers a topic name, the associated articles, and the general sentiment (see~\Cref{ch:5:triple}).
    
    \item \textbf{A domain-independent methodology for semantic triple extraction:} We propose our Semantic Triple Extraction Engine that exploits syntactic and grammatical structure of English news articles, without any dependency on existing knowledge base and ontologies, to extract triples of type (subject $\rightarrow$ predicate $\rightarrow$ object). These are extracted for each topic in a semantic cluster to provide insight into the information and sentiment surrounding key named entities in the article corpus of a topic (see~\Cref{ch:5:triple}).

    \item \textbf{A cohesive visualisation tool to display results:} A web-based application responsible for illustrating the results from the Topic Extraction Engine and Semantic Triple Extraction Engine in a coherent and cohesive visualisation by making use of circle-packing topic cluster diagrams and force-directed knowledge graphs respectively.


\end{enumerate}


Such a solution has great value and interest for not only the average consumer who does not want to sift through reams of news to gain a core insight into the developments within an industry but also for businesses that want to obtain an intelligent overview of key information within specific industries.

% way of evaluation
% ----------------------------------------

% The general idea in modelling this 'visualiser' entails a multitude of tasks. First of which, is identifying the key entities in relation to an industry by making use of entity extraction and building entity-relationship ontologies. These can be derived from large open-source knowledge bases (or knowledge graphs) such as Wikidata by leveraging the knowledge graph embeddings such as semantic triples. \hyperlink{8}{[8]}\hyperlink{9}{[9]} This can then, in turn, be visualised as a more succinct and relevant knowledge graph, now only showing selective information applicable to an industry. \\

% Next step is to apply Natural Language Processes (NLP) processes such as Named Entity Extraction (NER) for mining news articles to, again, identify the key entities in the industry, ma,king sure to avoid duplication and redundancy in these entities by using Named Entity Disambiguation (NED) and finally linking these entities to the knowledge graph using Named Entity Linking (NEL). The information from processing these news articles will augment and alter our knowledge graph based on the information obtained from these articles. There is an immense amount of research done on natural language processing using machine learning for mining data for entity extraction from articles \hyperlink{11}{[11]} and a range of existing state-of-the-art machine learning models such as ELMo-LSTM \hyperlink{31}{[31]} and RoBERTa developed by AllenNLP which are open-source. I will be using these models, evaluating their comparative performance and potentially using a hybrid of such models to best suit the needs of the project. \\

% Extracting the key entities and relationships from the news is useful but doesn't provide an overview of themes and events that dominate an industry during a period of time. This can be achieved by obtaining the sentiment and key themes seen in the news of a particular industry by using NLP processes like Sentiment Analysis \hyperlink{18}{[18]}\hyperlink{20}{[20]}\hyperlink{22}{[22]}\hyperlink{24}{[24]} and Topic Modelling \hyperlink{23}{[23]} respectively. Again, there is a range of research and pre-existing state-of-the-art sentiment analyser models, AllenNLP models can again be leveraged for this and evaluated in a similar fashion as those for entity extraction. \\

% The last milestone is to present all this information, the knowledge graph of entities, major themes/ topics and their associated sentiment, in an intelligent, interpretable and informative manner. This can be a particularly challenging task as it requires a lot of different considerations to be taken into account such as filtering what needs to be displayed, how it should be displayed and how to dynamically change the display based on new relevant information. Additionally, it can be challenging to identify what constitutes as a 'effective' visualisation as that can be fairly subjective. There is a range of different techniques and libraries that aid to data visualisation including libraries such as D3.js, Dash to build various types for graph visualisations as well as visual analyser clustering machine learning models such as Self-Organising Maps (Kohonen Maps). These different approaches allow for different capabilities and in turn, different ways of visualising the data. \\

% While there are numerous studies and research on these different aspects of the project, they usually approach a standalone use case e.g., getting sentiment from data. There is a need for integrating the different aspects of information mining and delivering cohesive visual platform for this information, allowing the user to see the changes in these industries over time.

\section{Ethical Considerations}
This project focuses on the semantic analysis of news articles with the goal of knowledge distillation, which is particularly beneficial from an intelligence standpoint because it condenses enormous amounts of data to present a visual overview of an industry for different genres of news across time.  Though our solution highlights recent developments and trends in a sector, it is highly unlikely that it will influence or propagate some radicalised opinion. Furthermore, given the absence of direct human involvement in our use case, there is no collection or direct use of personal data.

One key ethical aspect to consider is any bias introduced in our semantic analysis engine due to the nature of the input data, which may contain underlying bias. For example, if we use the engine with a dataset for the energy industry, which contains a large proportion of right-wing articles that mention themes like ``climate change being a hoax'', then this will be reflected in the output results (topics, associated sentiments and semantic triples) via the visualisation tool, indicating a negative sentiment associated with clean energy. This is the nature of news publication, which is more often than not, biased and polarising. This makes it very difficult to gauge what quanlifies as unbiased, and in fact, performing any filtration of articles to ``alleviate bias" would inherently introduce more bias.

Another key ethical consideration is that of using software and data with copyright licences. The usage of an online news article dataset is an important part of this study. This dataset is provided by Deep Search Labs and is scraped from online sources, particularly Bloomberg News. Using the articles (which are the Intellectual Property (IP) of the news companies) for ``non-commercial" data analysis is not a copyright infringement as the copyright law has been updated to provide an exception for ``Text and data mining technologies to help researchers process large amounts of data"~\cite{exceptions_to_copyright}. All other libraries and software packages used, such as D3, spaCy, Gensim and AllenNLP (see~\Cref{libraries}) are open-source and free to use for the purposes of this project (i.e., for academic use).